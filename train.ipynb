{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the seed for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "\n",
    "setup_seed(3407)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definet the device used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "gpu_list = [2]\n",
    "gpu_list_str = ','.join(map(str, gpu_list))\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", gpu_list_str)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `HyperGragh` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch_geometric.nn import GATv2Conv, LayerNorm\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from model.ViT import Mlp, VisionTransformer\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "\n",
    "class HypergraphNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(HypergraphNeuralNetwork, self).__init__()\n",
    "        self.conv1 = HypergraphConv(input_dim, hidden_dim1)\n",
    "        self.conv2 = HypergraphConv(hidden_dim1, output_dim)\n",
    "        #self.conv3 = HypergraphConv(hidden_dim2, output_dim)\n",
    "        self.norm = nn.LayerNorm(output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index, data.edge_attr)\n",
    "        x1 = F.dropout(F.relu(x), 0.3, training=True)\n",
    "        x2 = self.conv2(x1, data.edge_index, data.edge_attr)\n",
    "        #x2 = F.relu(x2)\n",
    "        #x3 = self.conv3(x2, data.edge_index, data.edge_attr)\n",
    "        x4 = self.norm(x2)\n",
    "\n",
    "        return x4\n",
    "\n",
    "\n",
    "def compute_adjacency_matrix(pos, adjacency_matrix):\n",
    "    num_edges = adjacency_matrix.size(1)\n",
    "    adjacency_matrix2 = torch.zeros((pos.size(0), pos.size(0)), dtype=torch.float)\n",
    "\n",
    "    for edge_index in range(num_edges):\n",
    "        node_a, node_b = adjacency_matrix[:, edge_index]\n",
    "        distance = torch.norm(pos[node_a] - pos[node_b])\n",
    "        adjacency_matrix2[node_a, node_b] = distance\n",
    "        adjacency_matrix2[node_b, node_a] = distance\n",
    "\n",
    "    max_distance = torch.max(adjacency_matrix2)\n",
    "    adjacency_matrix2 = adjacency_matrix2 / max_distance\n",
    "\n",
    "    nonzero_indices = torch.nonzero(adjacency_matrix2.sum(dim=1) > 0).squeeze()\n",
    "    adjacency_matrix2 = adjacency_matrix2[nonzero_indices, :]\n",
    "    adjacency_matrix2 = adjacency_matrix2[:, nonzero_indices]\n",
    "\n",
    "    return adjacency_matrix2\n",
    "\n",
    "\n",
    "def build_adj_hypergraph(features, adjacency_matrix, pos, threshold=0):\n",
    "    n, m = features.size()\n",
    "    device=features.device\n",
    "\n",
    "    hypergraph_edges = []\n",
    "    edge_weights = []\n",
    "\n",
    "    adjacency_matrix2 = compute_adjacency_matrix(pos, adjacency_matrix)\n",
    "\n",
    "    e, f = adjacency_matrix2.size()\n",
    "\n",
    "    for i in range(e):\n",
    "        Neighbor_distances = adjacency_matrix2[i, :]\n",
    "\n",
    "        valid_neighbors = (Neighbor_distances > threshold).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        for neighbor in valid_neighbors:\n",
    "            hypergraph_edges.append([i, neighbor.item()])\n",
    "            edge_weights.append(Neighbor_distances[neighbor].item())\n",
    "\n",
    "    hypergraph_edges = torch.tensor(hypergraph_edges, dtype=torch.long, device=device).t()\n",
    "    edge_weights = torch.tensor(edge_weights, dtype=torch.float, device=device)\n",
    "\n",
    "    data = Data(x=features, edge_index=hypergraph_edges, edge_attr=edge_weights, y=None)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `Tile Encoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_encoder = timm.create_model(\"hf_hub:prov-gigapath/prov-gigapath\", pretrained=False, checkpoint_path=\"../provgigapath/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `HPCell` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPCell(nn.Module):\n",
    "    def __init__(self, cell_dim=80, vit_depth=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tile_encoder = tile_encoder  # Feature extraction encoder\n",
    "        \n",
    "        # Define dimensions and configuration\n",
    "        self.dimension = 32 * 8\n",
    "        self.num_heads = 8\n",
    "        self.dropout_rate = 0.3\n",
    "\n",
    "        # Initialize Hypergraph Neural Network layer\n",
    "        self.hypergraph_layer = HypergraphNeuralNetwork(512, 128, 64, 256)\n",
    "\n",
    "        # Transformer for cell-level processing\n",
    "        self.transformer_model = VisionTransformer(\n",
    "            num_classes=cell_dim,\n",
    "            embed_dim=self.dimension,\n",
    "            depth=vit_depth,\n",
    "            mlp_head=True,\n",
    "            drop_rate=self.dropout_rate,\n",
    "            attn_drop_rate=self.dropout_rate\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for spot features\n",
    "        self.spot_fc_initial = Linear(1536, 512)\n",
    "        self.spot_fc_final = Linear(512, 256)\n",
    "\n",
    "        # Prediction heads\n",
    "        self.prediction_heads = nn.ModuleDict({\n",
    "            'spot': Mlp(256, 1024, cell_dim),\n",
    "            'local': Mlp(256, 1024, cell_dim),\n",
    "            'fusion': Mlp(256, 1024, cell_dim)\n",
    "        })\n",
    "\n",
    "    def forward(self, input_data, edges, positions):\n",
    "        # Extract spot features using encoder (frozen during inference)\n",
    "        with torch.no_grad():\n",
    "            self.tile_encoder.eval()\n",
    "            spot_features = self.tile_encoder(input_data).squeeze()\n",
    "\n",
    "        # Process spot features through FC layers\n",
    "        spot_features_transformed = self.spot_fc_initial(spot_features)\n",
    "\n",
    "        # Create hypergraph structure and extract local features\n",
    "        hypergraph_input = build_adj_hypergraph(spot_features_transformed, edges, positions)\n",
    "        local_features = self.hypergraph_layer(hypergraph_input).unsqueeze(0)\n",
    "\n",
    "        # Refine spot features\n",
    "        refined_spot_features = self.spot_fc_final(spot_features_transformed)\n",
    "\n",
    "        # Generate predictions from individual components\n",
    "        spot_prediction = self.prediction_heads['spot'](refined_spot_features)\n",
    "        local_features_squeezed = local_features.squeeze(0)\n",
    "        local_prediction = self.prediction_heads['local'](local_features_squeezed)\n",
    "\n",
    "        # Use transformer for global prediction\n",
    "        global_prediction, global_features = self.transformer_model(local_features)\n",
    "        global_prediction = global_prediction.squeeze()\n",
    "        global_features = global_features.squeeze()\n",
    "\n",
    "        # Combine features and compute fused prediction\n",
    "        combined_features = torch.mean(torch.stack([refined_spot_features, local_features_squeezed, global_features]), dim=0)\n",
    "        fused_prediction = self.prediction_heads['fusion'](combined_features)\n",
    "\n",
    "        # Average all predictions for final output\n",
    "        final_cell_prediction = torch.mean(torch.stack([\n",
    "            spot_prediction, local_prediction, global_prediction, fused_prediction\n",
    "        ]), dim=0)\n",
    "\n",
    "        return final_cell_prediction\n",
    "\n",
    "\n",
    "model = HPCell(vit_depth=3).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load train/test split file, here we train `HPCell` on other 3 donors in the humanlung cell2location dataset, and test `HPCell` on donnor A50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test slide files\n",
    "train_slides = [line.strip() for line in open(\"../train_test_splits/humanlung_cell2location/train_leave_A50.txt\")]\n",
    "test_slides = [line.strip() for line in open(\"../train_test_splits/humanlung_cell2location/test_leave_A50.txt\")]\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "def load_graphs(slides, data_path):\n",
    "    graph_list = [torch.load(os.path.join(data_path, f\"{item}.pt\")) for item in slides]\n",
    "    return Batch.from_data_list(graph_list)\n",
    "\n",
    "base_path = \"../data/humanlung_cell2location\"\n",
    "train_dataset = load_graphs(train_slides, base_path)\n",
    "test_dataset = load_graphs(test_slides, base_path)\n",
    "\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import torch_geometric\n",
    "torch_geometric.typing.WITH_PYG_LIB = False\n",
    "\n",
    "hop = 2\n",
    "subgraph_bs = 16\n",
    "loader_args = {\n",
    "    \"num_neighbors\": [-1] * hop,\n",
    "    \"batch_size\": subgraph_bs,\n",
    "    \"directed\": False,\n",
    "    \"num_workers\": 2,\n",
    "}\n",
    "\n",
    "train_loader = NeighborLoader(train_dataset, shuffle=True, input_nodes=None, **loader_args)\n",
    "test_loader = NeighborLoader(test_dataset, shuffle=False, input_nodes=None, **loader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `learning rate`, `criterion`, `optimizer` and `scheduler` used for training, we use `lr=1e-4` in our study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "\n",
    "lr = 1e-4\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "best_pearson = 0.0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train `HPCell` for 5 epochs, and save the checkpoint with the best Pearson R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "best_pearson = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"{'-' * 50}\\nEpoch: {epoch + 1}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    model.train()\n",
    "    train_metrics = {\"loss\": 0.0, \"samples\": 0, \"preds\": [], \"labels\": []}\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, y, edge_index, pos = batch.x.to(device), batch.y.to(device), batch.edge_index.to(device), batch.pos.to(device)\n",
    "        cell_labels = y[:, 250:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cell_preds = model(x, edge_index, pos)\n",
    "        loss = criterion(cell_preds, cell_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        center_size = len(batch.input_id)\n",
    "        train_metrics[\"loss\"] += loss.item() * center_size\n",
    "        train_metrics[\"samples\"] += center_size\n",
    "        train_metrics[\"preds\"].append(cell_preds[:center_size].cpu().detach().numpy())\n",
    "        train_metrics[\"labels\"].append(cell_labels[:center_size].cpu().detach().numpy())\n",
    "\n",
    "    train_loss = train_metrics[\"loss\"] / train_metrics[\"samples\"]\n",
    "    train_preds = np.concatenate(train_metrics[\"preds\"], axis=0)\n",
    "    train_labels = np.concatenate(train_metrics[\"labels\"], axis=0)\n",
    "\n",
    "    train_pearson_avg = np.mean([pearsonr(train_preds[:, i], train_labels[:, i])[0] for i in range(train_preds.shape[1])])\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_metrics = {\"loss\": 0.0, \"samples\": 0, \"preds\": [], \"labels\": []}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y, edge_index, pos = batch.x.to(device), batch.y.to(device), batch.edge_index.to(device), batch.pos.to(device)\n",
    "            cell_labels = y[:, 250:]\n",
    "            cell_preds = model(x, edge_index, pos)\n",
    "\n",
    "            loss = criterion(cell_preds, cell_labels)\n",
    "            center_size = len(batch.input_id)\n",
    "\n",
    "            test_metrics[\"loss\"] += loss.item() * center_size\n",
    "            test_metrics[\"samples\"] += center_size\n",
    "            test_metrics[\"preds\"].append(cell_preds[:center_size].cpu().detach().numpy())\n",
    "            test_metrics[\"labels\"].append(cell_labels[:center_size].cpu().detach().numpy())\n",
    "\n",
    "    test_loss = test_metrics[\"loss\"] / test_metrics[\"samples\"]\n",
    "    test_preds = np.concatenate(test_metrics[\"preds\"], axis=0)\n",
    "    test_labels = np.concatenate(test_metrics[\"labels\"], axis=0)\n",
    "\n",
    "    test_pearson_avg = np.mean([pearsonr(test_preds[:, i], test_labels[:, i])[0] for i in range(test_preds.shape[1])])\n",
    "\n",
    "    if test_pearson_avg > best_pearson:\n",
    "        best_pearson = test_pearson_avg\n",
    "        torch.save(model.state_dict(), \"../model_weights/A50.pth\")\n",
    "        print(f\"Model saved with best test Pearson correlation: {test_pearson_avg:.6f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Training Time: {elapsed_time // 60:.0f}m {elapsed_time % 60:.0f}s\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}, Train Pearson Avg: {train_pearson_avg:.6f}\")\n",
    "    print(f\"Test Loss: {test_loss:.6f}, Test Pearson Avg: {test_pearson_avg:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hist2Cell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
